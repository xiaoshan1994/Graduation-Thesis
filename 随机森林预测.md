```r
#v3考虑了个体效应，添加了省份这一factor变量（添加第二列并转换为factor) 年份无法添加 level数不一样
rm(list=ls()) #清空工作空间
#引用randomForest包
library(randomForest)
kai1_rf<-read.table('clipboard',header=T) #组合2文件
#将数据集分为训练集和测试集,比例为7:3
set.seed(6)
ind<- sample(2, nrow(kai1_rf), replace = TRUE, prob=c(0.7, 0.3))
traindata<- kai1_rf[ind == 1,]
testdata <- kai1_rf[ind == 2,]
train.y=traindata[,3]
train.x<-traindata[,c(1,4,5,6,7,8,9)]
test.y=testdata[,3]
test.x=testdata[,c(1,4,5,6,7,8,9)]
#训练模型 (Note: If xtest is given, defaults to FALSE)
result_rf<-randomForest(train.x, train.y, xtest=test.x, ytest=test.y, importance=TRUE, proximity=TRUE, keep.forest=TRUE)
#查看结果
result_rf
plot(result_rf) #树的数量与均方误差，可以看出在多少棵树时已经稳定了
#查看模型表现
importance(result_rf)
#用2017年数据预测并检验
kai_pre17<-read.table('clipboard',header=T) #2017年数据
pre17<-predict(result_rf,kai_pre17)

#计算Mean Squared Error（均方误差） ---  真实值和估计值之间的偏差平方和的平均值
mse_r_17<-mean((pre17-kai_pre17$lnCO2)^2)
#计算样本方差，样本为最初的大样本
var_lnCO2<- var(kai1_rf$lnCO2)
#计算拟合度R2
r2_pre17<-1-mse_r_17/var_lnCO2

#作图：2017年随机森林预测和真实值
x<-factor(kai_pre17$prov,level=c("BJ","TJ","HE","SX","NM","LN","JL","HLJ","SH","JS","ZJ","AH","FJ","JX","SD","HA","HB","HN","GD","GX","HI","CQ","SC","GZ","YN","SN","GS","QH","NX","XJ"))
y1<-pre17
y2<-kai_pre17$lnCO2
y3<-abs(y1-y2)
data.plot<-data.frame(x,y1,y2,y3)
p<-ggplot(data.plot)+geom_point(data=data.plot,aes(x= x,y= y1,colour="预测值"),size=1,group=1)+
geom_line(data=data.plot,aes(x= x,y= y1,colour="预测值"),group=1)+
geom_point(data=data.plot,aes(x=x, y=y2,colour="实际值"), size=1, group=2)+
geom_line(data=data.plot,aes(x=x, y=y2,colour="实际值"), group=2)+
scale_colour_manual("图例",values = c("预测值" = "#F8766D","实际值" = "#00BFC4","绝对误差"="#619CFF"))
p<-p+xlab("省份")+ylab("ln(CO2)")# 横纵坐标显示
p<-p+scale_y_continuous(limits=c(5.5,9), breaks=seq(5.5,9,0.5))# 调整y轴范围、间隔
p<-p+geom_text(data=data.plot,aes(x=x,y=y2,label = round(y3,2),colour="绝对误差"),size=3.5,nudge_x=-0.6)#显示绝对误差
p<-p+coord_fixed(ratio=5)#调整y坐标轴与x的比例，默认是1

# p<-p+geom_text(data=data.plot,aes(x=x,y=y1,label = round(y1,2),colour="预测值"),size=2,nudge_x=-0.4)+
geom_text(data=data.plot,aes(x=x,y=y2,label = round(y2,2),colour="实际值"),size=2,nudge_x=0.4)# 显示具体点的数值，注意映射内容不能少，因为ggplot()中无映射

#2018年随机森林预测结果,注意聚类结果出来后要按照结果分组画图
kai_pre18<-read.table('clipboard',header=T) #2018年数据
pre18<-predict(result_rf,kai_pre18)

#直方图预测图：2018年的CO2
x<-factor(kai_pre18$prov,level=c("BJ","TJ","HE","SX","NM","LN","JL","HLJ","SH","JS","ZJ","AH","FJ","JX","SD","HA","HB","HN","GD","GX","HI","CQ","SC","GZ","YN","SN","GS","QH","NX","XJ"))
y3<-exp(pre18)
y4<-c("分组1","分组4","分组2","分组3","分组4","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2")
y5<-pre18
data.plot<-data.frame(x,y3,y4,y5)
#做y1的柱状图（x轴中x按照y3从小到大的顺序排序，width调整单个柱子的宽度，默认是0.9，最大是1；geom_text给柱子加数值，round用来保留小数位数，size是数值大小，vjust调整位置，在中间是0.5。注：text的映射是实际值而标注是取对数数值，方便显示。
ggplot(data.plot, aes(x = reorder(x, y3), y=y3,fill=y4)) + geom_bar(stat = 'identity',width=-0.9)+geom_text(aes(x=x,y=y3,label = round(y5,2)),size=4,vjust=0.5,angle=0)+scale_fill_brewer(name="hahaha", palette = "Pastel1")+xlab("不易啊")+ylab("生活")+scale_y_continuous(limits=c(0,7000), breaks=seq(0,7000,1000))

kai_pre18<-read.table('clipboard',header=T) #2018年数据
pre18<-predict(result_rf,kai_pre18)

#金字塔预测图：2018年的CO2，参考https://zhuanlan.zhihu.com/p/29101316（美美的商务范儿——蝴蝶图）,后期用的PS
x<-factor(kai_pre18$prov,level=c("BJ","TJ","HE","SX","NM","LN","JL","HLJ","SH","JS","ZJ","AH","FJ","JX","SD","HA","HB","HN","GD","GX","HI","CQ","SC","GZ","YN","SN","GS","QH","NX","XJ"))
y3<-exp(pre18)
#y4<-c("分组1","分组4","分组2","分组3","分组4","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2","分组3","分组4","分组1","分组2")
y4<-kai_pre18$Cluster #聚类的因子变量列
y5<-pre18
data.plot<-data.frame(x,y3,y4,y5)
library(ggplot2)
p1<-ggplot(data.plot)+
geom_hline(yintercept=mean(y3),linetype=2,size=0.8,colour="grey")+
geom_bar(aes(x= reorder(x, -y3),y=y3,fill=y4),stat="identity")+
ylim(-200,8000)+
geom_text(aes(x="HI", y=mean(y3)+500 ,label=round(mean(y3),2)),colour="darkgrey")+
geom_text(aes(x= reorder(x, -y3),y=-200,label=x),vjust=.5)+
geom_text(aes(x= reorder(x, -y3),y=y3+400,label=round(y3,2), size=3.5))+
coord_flip()+
theme_void()
p1

#importance 两个散点图
library(ggplot2)
rm<-row.names(importance(result_rf)) #由于第一列没有列名，只能将行名提取
x_f<-factor(rm, level=c("prov","lnUP","lnGDP","lnCLA","lnGSA","RD","lnBD"))#将变量因子化并按指定顺序排序
y1<-importance(result_rf)[,1] 
y2<-importance(result_rf)[,2] #将二三列数据分别赋值给y1和y2
data.plot<-data.frame(x_f, y1,y2) #建好数据库
#做y1和y2的散点图（size调整散点的大小；geom_text给散点加数值，round用来保留小数位数，size是数值大小，vjust调整位置，在中间是0.5；scale_y_continuous(）或scale_x_continuous用来调整坐标轴连续变量的范围和区间）
h1<-ggplot(data.plot, aes(x=reorder(x_f, y1), y=y1,colour=x_f)) +geom_point(size=3)+geom_text(aes(label = round(y1,2)),size=6,vjust=-0.6,show.legend = FALSE)+scale_fill_brewer(name="hahaha", palette = "Pastel1")+scale_y_continuous(limits=c(0,30), breaks=seq(0,30,5))+xlab("")+ylab("")
h2<-ggplot(data.plot, aes(x=reorder(x_f, y2), y=y2,colour=x_f)) +geom_point(size=3)+geom_text(aes(label = round(y2,2)),size=6,vjust=-0.6,show.legend = FALSE)+scale_fill_brewer(name="hahaha", palette = "Pastel1")+scale_y_continuous(limits=c(0,30), breaks=seq(0,30,5))+xlab("")+ylab("")
#组合h1与h2
library(customLayout)
mylay <- lay_new(mat = matrix(1:2, ncol = 2))
plot_list <- list(h1,h2) 
lay_grid(plot_list, mylay)

#利用第三章的线性回归模型预测
 kai1_lm<-read.table('clipboard',header=T) #组合2文件
result_lm<-lm(lnCO2~lnUP+lnGDP+lnCLA+lnGSA+RD+lnBD,data=kai1_lm)
summary(result_lm)#结果同plm混合面板
#预测17年
kai_pre17<-read.table('clipboard',header=T) #2017年数据
pre17_lm<-predict(result_lm,kai_pre17)
pre17_lm
#计算Mean Squared Error（均方误差） ---  真实值和估计值之间的偏差平方和的平均值
mse_l_17<-mean((pre17_lm-kai_pre17$lnCO2)^2)
mse_l_17
#计算样本方差，样本为最初的大样本
kai1<-read.table('clipboard',header=T) #组合2数据
var_lnCO2<- var(kai1$lnCO2)
#计算线性回归的拟合度R2
r2_pre17_lm<-1-mse_l_17/var_lnCO2
r2_pre17_lm

#作图：2017年线性回归、随机森林和真实值结果对比
x<-factor(kai_pre17$prov,level=c("BJ","TJ","HE","SX","NM","LN","JL","HLJ","SH","JS","ZJ","AH","FJ","JX","SD","HA","HB","HN","GD","GX","HI","CQ","SC","GZ","YN","SN","GS","QH","NX","XJ"))
y1<-pre17_lm
y2<-kai_pre17$lnCO2
y3<-abs(y1-y2)
#y4<-c(8.120803 ,7.740497 ,8.271564 ,7.726400 ,8.264060 ,8.476691, 7.561925, 8.459384, 7.913806, 8.345603, 8.061862 ,7.720095 ,7.695268, 7.421546 ,8.597868 ,8.098203,7.962250, 7.846419 ,8.840596 ,7.468991 ,5.863860 ,7.584310 ,8.139188 ,7.486594,7.208093, 7.805922 ,7.069626 ,5.998996 ,6.092303 ,7.815892)
y4<-pre17_rf
y5<-abs(y4-y2)
data.plot<-data.frame(x,y1,y2,y3,y4,y5)
p<-ggplot(data.plot)+geom_point(data=data.plot,aes(x= x,y= y1,colour="linear"),size=1,group=1)+
geom_line(data=data.plot,aes(x= x,y= y1,colour="linear"),group=1)+
geom_point(data=data.plot,aes(x=x, y=y4,colour="randomForest"), size=1, group=3)+
geom_line(data=data.plot,aes(x=x, y=y4,colour="randomForest"), group=3)+
geom_point(data=data.plot,aes(x=x, y=y2,colour="实际值"), size=1, group=2)+
geom_line(data=data.plot,aes(x=x, y=y2,colour="实际值"), group=2)+
scale_colour_manual("图例",values = c("linear" = "#F8766D","randomForest" = "#00BFC4","实际值"="#619CFF"))+
scale_fill_manual("图例",values = c("linear" = "#F8766D","randomForest" = "#00BFC4","实际值"="#619CFF"))+
geom_bar(data=data.plot,aes(x=x, y=10*y3,fill="linear"),stat = "identity")+
geom_bar(data=data.plot,aes(x=x, y=10*y5,fill="randomForest"),stat = "identity")
p<-p+scale_y_continuous(limits=c(0,10), breaks=seq(0,10,1))# 调整y轴范围、间隔
#p<-p+coord_fixed(ratio=3)#调整y坐标轴与x的比例，默认是1
p<-p+theme(text=element_text(size=20))#整体字体

#寻找最合适的mytry

n <- 7
rsq <- c(1)
for (i in 1:n){  
 m<-randomForest(train.x, train.y, xtest=test.x, ytest=test.y, importance=TRUE, proximity=TRUE, keep.forest=TRUE,ntree=500,mtry=i)
err<-mean(m$rsq)
 rsq[i] <- err  
}
print(rsq)

n <- 7
mse <- c(1)
for (i in 1:n){  
 m<-randomForest(train.x, train.y, xtest=test.x, ytest=test.y, importance=TRUE, proximity=TRUE, keep.forest=TRUE,ntree=500,mtry=i)
err<-mean(m$mse)
 mse[i] <- err  
} 
print(mse)



#结果记录
 randomForest(x = train.x, y = train.y, xtest = test.x, ytest = test.y,      importance = TRUE, proximity = TRUE, keep.forest = TRUE) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 2

          Mean of squared residuals: 0.02357374
                    % Var explained: 96.45
                       Test set MSE: 0.02
                    % Var explained: 94.85

       %IncMSE IncNodePurity
prov  20.34588    26.0084564
lnUP  15.50914    20.1595895
lnGDP 13.83448     4.1438202
lnCLA 15.64870    17.5286564
lnGSA 12.22344    10.6382145
RD    11.41541     0.9701275
lnBD  13.22054    13.3174047

 pre17
   1        2        3        4        5        6        7        8 
8.120803 7.740497 8.271564 7.726400 8.264060 8.476691 7.561925 8.459384 
       9       10       11       12       13       14       15       16 
7.913806 8.345603 8.061862 7.720095 7.695268 7.421546 8.597868 8.098203 
      17       18       19       20       21       22       23       24 
7.962250 7.846419 8.840596 7.468991 5.863860 7.584310 8.139188 7.486594 
      25       26       27       28       29       30 
7.208093 7.805922 7.069626 5.998996 6.092303 7.815892 

mse_r_17
[1] 0.01646267
r2_pre17
[1] 0.972971

#线性回归方法预测结果
> pre17_lm<-predict(result_lm,kai_pre17)
> pre17_lm
       1        2        3        4        5        6        7        8        9       10       11       12 
7.956721 7.665557 8.211792 7.387480 7.884794 8.337915 7.747346 7.939037 8.175986 8.421300 7.946821 8.005032 
      13       14       15       16       17       18       19       20       21       22       23       24 
7.927900 7.850517 8.744719 8.582918 8.371242 8.293115 9.023644 7.659540 6.145550 7.507697 8.297016 7.401709 
      25       26       27       28       29       30 
7.624668 7.833900 7.056074 5.668619 6.327631 7.543487 
> mse_l_17<-mean((pre17_lm-kai_pre17$lnCO2)^2)
> mse_l_17
[1] 0.1053262
> var_lnCO2
[1] 0.6090744
> r2_pre17_lm<-1-mse_l_17/var_lnCO2
> r2_pre17_lm
[1] 0.8270717

#mtry
> print(rsq)
[1] 0.9393580 0.9647015 0.9662801 0.9717779 0.9720023 0.9716624 0.9728816
[1] 0.9403389 0.9645718 0.9696136 0.9684964 0.9681416 0.9712857 0.9718748

> print(mse)
[1] 0.03904194 0.02492405 0.01939703 0.01837311 0.01868182 0.01766080
[7] 0.01864866
```
